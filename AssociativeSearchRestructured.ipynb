{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import spacy\n",
    "import random\n",
    "import requests\n",
    "import pandas              as pd\n",
    "import numpy               as np\n",
    "import gensim.downloader   as api\n",
    "from collections           import Counter, deque\n",
    "from itertools             import combinations, chain, permutations\n",
    "from time                  import perf_counter\n",
    "from functools             import wraps, singledispatch\n",
    "from gensim.models         import KeyedVectors\n",
    "from gensim.utils          import simple_preprocess\n",
    "from dataclasses           import dataclass\n",
    "from bs4                   import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_or_en = \"pl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.szczerbinski\\Anaconda3\\lib\\site-packages\\spacy\\util.py:730: UserWarning: [W095] Model 'pl_core_news_sm' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy loaded\n",
      "word2vec loaded\n"
     ]
    }
   ],
   "source": [
    "if pl_or_en == \"pl\":\n",
    "    nlp = spacy.load(\"pl_core_news_sm\", exclude=[\"ner\"])\n",
    "    print(\"spaCy loaded\")\n",
    "    word2vec = KeyedVectors.load(\"word2vec_100_3_polish.bin\")\n",
    "    print(\"word2vec loaded\")\n",
    "    \n",
    "else:\n",
    "    word2vec = api.load(\"glove-wiki-gigaword-100\")\n",
    "    print(\"word2vec loaded\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])\n",
    "    print(\"spaCy loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SentenceSeries:\n",
    "    ds:pd.Series\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        ds_name = self.ds.name\n",
    "        self.pre_sentences = self.ds.apply(lambda row: \" \".join([x for x in row.split() if str(x[0]) !=\"#\"]))\n",
    "        self.pre_sentences = pd.Series(nlp.pipe(self.ds.to_list())) #spacy pipeline \n",
    "        self.sentences = (self.pre_sentences.apply(lambda row: [*row.sents]) #populate series with lists of sentences\n",
    "                                           .apply(lambda row: [[w.lemma_ for w in list(filter(lambda y: y.pos_ in {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\"}, x)) \n",
    "                                                                if SentenceSeries.lemma_criteria(w.lemma_)] for x in row]))\n",
    "        self.sentences.columns = [ds_name]\n",
    "        self.all_sentences = self.sentences.explode()\n",
    "\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def lemma_criteria(lemma):\n",
    "        return (len(str(lemma)) > 2\n",
    "                and len(str(lemma)) < 20\n",
    "                and str(lemma).isalnum()\n",
    "                and str(lemma) not in Corpus.stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        \n",
    "    def __call__(self):\n",
    "        try:\n",
    "            synonyms = [x[0] for x in word2vec.similar_by_word(self.word, topn=20)]\n",
    "            print(\"word2vec synonyms:\", synonyms)\n",
    "            print(\"-\"*50)\n",
    "        except KeyError:\n",
    "            synonyms = [self.word]    \n",
    "        \n",
    "#         synonyms = {*synonyms, *self.scrape_synonyms(), self.word}\n",
    "        synonyms = {*self.scrape_synonyms(), self.word}\n",
    "        synonyms = {x for x in synonyms if len(x) > 2 and x.isalnum() and x not in Corpus.stopwords and x!=\"kupa\"}\n",
    "        \n",
    "        for x in sorted(synonyms):\n",
    "            print(x)\n",
    "        print(\"-\"*50)\n",
    "        return {self.word : synonyms}\n",
    "    \n",
    "    def synonyms_scraper(self, lang=pl_or_en):\n",
    "        '''\n",
    "        API calls for more synonyms/contextually related phrases - synonyms.reverso.net\n",
    "        '''\n",
    "        USERAGENTS = (\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1\", \n",
    "        \"Mozilla/5.0 (Windows NT 6.3; rv:36.0) Gecko/20100101 Firefox/36.0\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10; rv:33.0) Gecko/20100101 Firefox/33.0\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\",\n",
    "        )\n",
    "\n",
    "        ua = USERAGENTS[random.randrange(len(USERAGENTS))]\n",
    "        headers = {'user-agent': ua}\n",
    "        url = f\"https://synonyms.reverso.net/synonim/{lang}/{self.word}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        synonyms = [x.text for x in soup.select(\"a.synonym\")][:16]\n",
    "       \n",
    "        return synonyms\n",
    "    \n",
    "    def scrape_synonyms(self, second_lang=\"en\"):\n",
    "        synonyms = self.synonyms_scraper()\n",
    "        if len(synonyms) == 0:\n",
    "            synonyms = self.synonyms_scraper(lang=second_lang)\n",
    "        print(\"Scraped synonyms:\", synonyms)\n",
    "        print(\"-\"*50)\n",
    "        return synonyms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    \n",
    "    stopwords = {x.split(\"\\n\")[0] for x in open(f\"stopwords_{pl_or_en}.txt\")}\n",
    "    \n",
    "    \n",
    "    def __init__(self, data,*, textcol=\"Treść wypowiedzi\"):\n",
    "        self.data = data\n",
    "        self.textcol = textcol\n",
    "        self.data = self.data.fillna(\"0\")\\\n",
    "                             .drop_duplicates(subset=textcol)\\\n",
    "                             .reset_index(drop=True)\n",
    "                \n",
    "        if \"Rodzaj wzmianki\" in self.data.columns:\n",
    "            self.data = self.data[~self.data[\"Rodzaj wzmianki\"].isin([\"Artykuł\"])].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def from_csv(cls, filename, sample=1000):\n",
    "        try:\n",
    "            df = pd.read_csv(f\"{filename}.csv\", sep=\";\", nrows=sample)\n",
    "        except pd.io.parsers.ParserError:\n",
    "            df = pd.read_csv(f\"{filename}.csv\", nrows=sample)\n",
    "        return Corpus(df)\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def from_xlsx(cls, filename, sample=1000):\n",
    "        df = pd.read_excel(f\"~/desktop/{filename}.xlsx\", nrows=sample, engine='openpyxl')\n",
    "        return Corpus(df)\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"A dataset of {len(self.data)} records\"\n",
    "    \n",
    "    \n",
    "    def timer(fn):\n",
    "        @wraps(fn)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            s = perf_counter()\n",
    "            func = fn(*args, **kwargs)\n",
    "            e = perf_counter()\n",
    "            print(f\"{fn.__name__} ran in {e-s: .2f}s\")\n",
    "            return func\n",
    "        return wrapper\n",
    "    \n",
    "    \n",
    "    @timer\n",
    "    def create_lemmata(self):\n",
    "        sentence_object = SentenceSeries(self.data[self.textcol])\n",
    "        self.sents = sentence_object.sentences\n",
    "        self.all_sents = sentence_object.all_sentences.fillna(\"None\")\n",
    "        self.raw_sentences = pd.Series(sentence_object.pre_sentences.apply(lambda row: [*row.sents]).values)\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def top_words(self):\n",
    "        return Counter(chain.from_iterable(self.lemmata.values)).most_common(100)\n",
    "    \n",
    "    \n",
    "    def top_ngrams(self, nfunc):\n",
    "        '''\n",
    "         c.top_ngrams(nltk.bigrams)\n",
    "         c.top_ngrams(nltk.trigrams)\n",
    "        '''\n",
    "        all_grams = chain.from_iterable(self.lemmata.apply(lambda row : [*nfunc(row)]).values)\n",
    "        return Counter(all_grams).most_common(100)\n",
    "    \n",
    "    \n",
    "    def filter_similar(self, word):\n",
    "        filtered = self.lemmata[self.lemmata.apply(lambda x : word in x)]\n",
    "        return {x[0] for x in Counter(chain.from_iterable(filtered.values)).most_common(10)}\n",
    "    \n",
    "    @timer\n",
    "    def associative_search(self, *words, bigram=False, to_excel=False):\n",
    "        '''\n",
    "        For the time being, a fairly convoluted implementation of the actual associative search engine.\n",
    "        '''\n",
    "        all_sents_df = pd.DataFrame(self.all_sents)\n",
    "        all_sents_df.columns = [self.textcol]\n",
    "        associated_words = {*chain.from_iterable([list(Word(word)().values())[0] for word in words])}\n",
    "        containing_df = all_sents_df[all_sents_df[self.textcol].apply(lambda x:  any([str(y).lower() in associated_words for y in x]))]\n",
    "        positions = [*set(containing_df.index)]\n",
    "        recombined_df = pd.DataFrame(containing_df.groupby(containing_df.index)[self.textcol].apply(lambda x:','.join(x.astype(str))))\n",
    "        recombined_df[\"tag\"] = recombined_df[self.textcol]\n",
    "        del recombined_df[self.textcol]\n",
    "        recombined_df = pd.concat([self.data.take(positions), recombined_df], axis=1)\n",
    "        recombined_df[\"tag\"] = recombined_df[\"tag\"].apply(lambda row: row.lower())\\\n",
    "                                                   .apply(lambda row: [x for x in associated_words if x in row])\n",
    "        \n",
    "        if to_excel:\n",
    "            recombined_df.to_excel(f\"{'_'.join(words)}_associated_search_res.xlsx\")\n",
    "    \n",
    "#         df = pd.concat([recombined_df, self.sents.apply(lambda x: [*chain.from_iterable(x)])], axis=1)\n",
    "\n",
    "\n",
    "        return recombined_df\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Corpus.from_csv(\"leroy\", sample=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_lemmata ran in  10.97s\n"
     ]
    }
   ],
   "source": [
    "c.create_lemmata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec synonyms: ['szafa', 'komoda', 'kredens', 'szuflada', 'skrzynka', 'schowek', 'szafeczka', 'półka', 'stojak', 'lodówka', 'komódka', 'toaletka', 'półeczka', 'kasetka', 'przegródka', 'skrzyneczka', 'regał', 'bieliźniarka', 'pudełko', 'wieszak']\n",
      "--------------------------------------------------\n",
      "Scraped synonyms: ['schowek', 'kredens', 'gabinet', 'szafy', 'szafie', 'garderoba', 'szafę', 'szuflada', 'komodzie', 'klozecie', 'firma', 'biuro', 'magazyn', 'skrytka', 'praktyka', 'klinika']\n",
      "--------------------------------------------------\n",
      "biuro\n",
      "gabinet\n",
      "garderoba\n",
      "klinika\n",
      "klozecie\n",
      "komodzie\n",
      "kredens\n",
      "magazyn\n",
      "praktyka\n",
      "schowek\n",
      "skrytka\n",
      "szafie\n",
      "szafka\n",
      "szafy\n",
      "szafę\n",
      "szuflada\n",
      "--------------------------------------------------\n",
      "associative_search ran in  0.61s\n"
     ]
    }
   ],
   "source": [
    "result = c.associative_search(\"szafka\", bigram=False, to_excel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treść wypowiedzi</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Post 17. Dzisiaj napiszę o powstawaniu mebli kuchennych :) 🧑‍🍳👩‍🍳. Zrobiliśmy konstrukcje- boki ...</td>\n",
       "      <td>[szuflada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Zdecydowanie odradzam, meble źle wymierzoneniedopasowane, szary między szafkami dolna i górną a ...</td>\n",
       "      <td>[szafka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Dzień dobry.\\n\\nDzisiaj mam dla Was magiczną sypialnię w której króluje tapeta od UbierzSwojeŚci...</td>\n",
       "      <td>[szafka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Witam,\\n\\nDzisiaj mam dla Was przepiękna wizualizację sypialni przy użyciu tapety od UbierzSwoje...</td>\n",
       "      <td>[szafka, garderoba]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Witajcie w Naszej bajce 😁💥\\n\\nDługo zwlekałam, żeby założyć ten profil,ale w końcu się udało! Od...</td>\n",
       "      <td>[praktyka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Dobry Wieczór Obecnym!\\n\\nW dzisiejszym poście chce Wam przedstawić a raczej pochwalić się jak n...</td>\n",
       "      <td>[szafka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>🌎 Od 17 października będziecie mogli zobaczyć w CMWŁ wystawę „Ziemia (P)oddana”. Stanowi ona pod...</td>\n",
       "      <td>[magazyn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>Dawno takiego urwania głowy nie miałam i to nie związanego z moją pracą 😊 po nowym roku czekają ...</td>\n",
       "      <td>[szafka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>Najlepiej meble na wymiar. Meble takie będą oczywiście droższe od mebli gotowych, ale kuchni nie...</td>\n",
       "      <td>[szafka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>Promocja Noworoczna 🥂\\n⠀\\nMieszkania w budynkach 6 i 7 są teraz tańsze nawet o 17 000 zł ⚠️\\n⠀\\n...</td>\n",
       "      <td>[biuro]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>Nowy Rok rozpoczynamy skończoną łazienką! 🛀\\n\\nZachowaliśmy tu klimat bardziej nowoczesny. Królu...</td>\n",
       "      <td>[szafka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>Kolejny element układanki pojawił się w gabinecie. Chodzi tu oczywiście o dywan, którego znalezi...</td>\n",
       "      <td>[gabinet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>A tutaj mała zapowiedź zmian w kuchni 💙\\nPo drugiej stronie zmieniło się wszystko, czekam tylko ...</td>\n",
       "      <td>[szafka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>U nas po Tesco będzie niedługo Leroy Merlin Soprano 09/07/2020 10:11 W czerwcu było jeszcze 5Kap...</td>\n",
       "      <td>[kredens]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>Dziś w krótkim wideo przedstawiamy Wam 10-letnią Michalinę – jedną z kuratorek wystawy „Ziemia (...</td>\n",
       "      <td>[magazyn]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                        Treść wypowiedzi  \\\n",
       "33   Post 17. Dzisiaj napiszę o powstawaniu mebli kuchennych :) 🧑‍🍳👩‍🍳. Zrobiliśmy konstrukcje- boki ...   \n",
       "40   Zdecydowanie odradzam, meble źle wymierzoneniedopasowane, szary między szafkami dolna i górną a ...   \n",
       "155  Dzień dobry.\\n\\nDzisiaj mam dla Was magiczną sypialnię w której króluje tapeta od UbierzSwojeŚci...   \n",
       "182  Witam,\\n\\nDzisiaj mam dla Was przepiękna wizualizację sypialni przy użyciu tapety od UbierzSwoje...   \n",
       "226  Witajcie w Naszej bajce 😁💥\\n\\nDługo zwlekałam, żeby założyć ten profil,ale w końcu się udało! Od...   \n",
       "292  Dobry Wieczór Obecnym!\\n\\nW dzisiejszym poście chce Wam przedstawić a raczej pochwalić się jak n...   \n",
       "300  🌎 Od 17 października będziecie mogli zobaczyć w CMWŁ wystawę „Ziemia (P)oddana”. Stanowi ona pod...   \n",
       "405  Dawno takiego urwania głowy nie miałam i to nie związanego z moją pracą 😊 po nowym roku czekają ...   \n",
       "458  Najlepiej meble na wymiar. Meble takie będą oczywiście droższe od mebli gotowych, ale kuchni nie...   \n",
       "526  Promocja Noworoczna 🥂\\n⠀\\nMieszkania w budynkach 6 i 7 są teraz tańsze nawet o 17 000 zł ⚠️\\n⠀\\n...   \n",
       "552  Nowy Rok rozpoczynamy skończoną łazienką! 🛀\\n\\nZachowaliśmy tu klimat bardziej nowoczesny. Królu...   \n",
       "567  Kolejny element układanki pojawił się w gabinecie. Chodzi tu oczywiście o dywan, którego znalezi...   \n",
       "640  A tutaj mała zapowiedź zmian w kuchni 💙\\nPo drugiej stronie zmieniło się wszystko, czekam tylko ...   \n",
       "704  U nas po Tesco będzie niedługo Leroy Merlin Soprano 09/07/2020 10:11 W czerwcu było jeszcze 5Kap...   \n",
       "818  Dziś w krótkim wideo przedstawiamy Wam 10-letnią Michalinę – jedną z kuratorek wystawy „Ziemia (...   \n",
       "\n",
       "                     tag  \n",
       "33            [szuflada]  \n",
       "40              [szafka]  \n",
       "155             [szafka]  \n",
       "182  [szafka, garderoba]  \n",
       "226           [praktyka]  \n",
       "292             [szafka]  \n",
       "300            [magazyn]  \n",
       "405             [szafka]  \n",
       "458             [szafka]  \n",
       "526              [biuro]  \n",
       "552             [szafka]  \n",
       "567            [gabinet]  \n",
       "640             [szafka]  \n",
       "704            [kredens]  \n",
       "818            [magazyn]  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[[\"Treść wypowiedzi\", 'tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b88c8b3aef7e8088e0963b11b034c347091d77e3519f3a60d33afcaa6580504b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
