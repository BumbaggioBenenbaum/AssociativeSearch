{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import spacy\n",
    "import random\n",
    "import requests\n",
    "import pandas              as pd\n",
    "import numpy               as np\n",
    "import gensim.downloader   as api\n",
    "from collections           import Counter, deque\n",
    "from itertools             import combinations, chain, permutations\n",
    "from time                  import perf_counter\n",
    "from functools             import wraps, singledispatch\n",
    "from gensim.models         import KeyedVectors\n",
    "from gensim.utils          import simple_preprocess\n",
    "from dataclasses           import dataclass\n",
    "from bs4                   import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_or_en = \"pl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.szczerbinski\\Anaconda3\\lib\\site-packages\\spacy\\util.py:730: UserWarning: [W095] Model 'pl_core_news_sm' (3.0.0) was trained with spaCy v3.0 and may not be 100% compatible with the current version (3.1.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy loaded\n",
      "word2vec loaded\n"
     ]
    }
   ],
   "source": [
    "if pl_or_en == \"pl\":\n",
    "    nlp = spacy.load(\"pl_core_news_sm\", exclude=[\"ner\"])\n",
    "    print(\"spaCy loaded\")\n",
    "    word2vec = KeyedVectors.load(\"word2vec_100_3_polish.bin\")\n",
    "    print(\"word2vec loaded\")\n",
    "    \n",
    "else:\n",
    "    word2vec = api.load(\"glove-wiki-gigaword-100\")\n",
    "    print(\"word2vec loaded\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])\n",
    "    print(\"spaCy loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SentenceSeries:\n",
    "    ds:pd.Series\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        ds_name = self.ds.name\n",
    "        self.pre_sentences = self.ds.apply(lambda row: \" \".join([x for x in row.split() if str(x[0]) !=\"#\"]))\n",
    "        self.pre_sentences = pd.Series(nlp.pipe(self.ds.to_list())) #spacy pipeline \n",
    "        self.sentences = (self.pre_sentences.apply(lambda row: [*row.sents]) #populate series with lists of sentences\n",
    "                                           .apply(lambda row: [[w.lemma_ for w in list(filter(lambda y: y.pos_ in {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\"}, x)) \n",
    "                                                                if SentenceSeries.lemma_criteria(w.lemma_)] for x in row]))\n",
    "        self.sentences.columns = [ds_name]\n",
    "        self.all_sentences = self.sentences.explode()\n",
    "\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def lemma_criteria(lemma):\n",
    "        return (len(str(lemma)) > 2\n",
    "                and len(str(lemma)) < 20\n",
    "                and str(lemma).isalnum()\n",
    "                and str(lemma) not in Corpus.stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        \n",
    "    def __call__(self):\n",
    "        try:\n",
    "            synonyms = [x[0] for x in word2vec.similar_by_word(self.word, topn=20)]\n",
    "            print(\"word2vec synonyms:\", synonyms)\n",
    "            print(\"-\"*50)\n",
    "        except KeyError:\n",
    "            synonyms = [self.word]    \n",
    "        \n",
    "#         synonyms = {*synonyms, *self.scrape_synonyms(), self.word}\n",
    "        synonyms = {*self.scrape_synonyms(), self.word}\n",
    "        synonyms = {x for x in synonyms if len(x) > 2 and x.isalnum() and x not in Corpus.stopwords and x!=\"kupa\"}\n",
    "        \n",
    "        for x in sorted(synonyms):\n",
    "            print(x)\n",
    "        print(\"-\"*50)\n",
    "        return {self.word : synonyms}\n",
    "    \n",
    "    def synonyms_scraper(self, lang=pl_or_en):\n",
    "        '''\n",
    "        API calls for more synonyms/contextually related phrases - synonyms.reverso.net\n",
    "        '''\n",
    "        USERAGENTS = (\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1\", \n",
    "        \"Mozilla/5.0 (Windows NT 6.3; rv:36.0) Gecko/20100101 Firefox/36.0\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10; rv:33.0) Gecko/20100101 Firefox/33.0\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\",\n",
    "        )\n",
    "\n",
    "        ua = USERAGENTS[random.randrange(len(USERAGENTS))]\n",
    "        headers = {'user-agent': ua}\n",
    "        url = f\"https://synonyms.reverso.net/synonim/{lang}/{self.word}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        synonyms = [x.text for x in soup.select(\"a.synonym\")][:16]\n",
    "       \n",
    "        return synonyms\n",
    "    \n",
    "    def scrape_synonyms(self, second_lang=\"en\"):\n",
    "        synonyms = self.synonyms_scraper()\n",
    "        if len(synonyms) == 0:\n",
    "            synonyms = self.synonyms_scraper(lang=second_lang)\n",
    "        print(\"Scraped synonyms:\", synonyms)\n",
    "        print(\"-\"*50)\n",
    "        return synonyms\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    \n",
    "    stopwords = {x.split(\"\\n\")[0] for x in open(f\"stopwords_{pl_or_en}.txt\")}\n",
    "    \n",
    "    \n",
    "    def __init__(self, data,*, textcol=\"TreÅ›Ä‡ wypowiedzi\"):\n",
    "        self.data = data\n",
    "        self.textcol = textcol\n",
    "        self.data = self.data.fillna(\"0\")\\\n",
    "                             .drop_duplicates(subset=textcol)\\\n",
    "                             .reset_index(drop=True)\n",
    "                \n",
    "        if \"Rodzaj wzmianki\" in self.data.columns:\n",
    "            self.data = self.data[~self.data[\"Rodzaj wzmianki\"].isin([\"ArtykuÅ‚\"])].reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def from_csv(cls, filename, sample=1000):\n",
    "        try:\n",
    "            df = pd.read_csv(f\"{filename}.csv\", sep=\";\", nrows=sample)\n",
    "        except pd.io.parsers.ParserError:\n",
    "            df = pd.read_csv(f\"{filename}.csv\", nrows=sample)\n",
    "        return Corpus(df)\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def from_xlsx(cls, filename, sample=1000):\n",
    "        df = pd.read_excel(f\"~/desktop/{filename}.xlsx\", nrows=sample, engine='openpyxl')\n",
    "        return Corpus(df)\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"A dataset of {len(self.data)} records\"\n",
    "    \n",
    "    \n",
    "    def timer(fn):\n",
    "        @wraps(fn)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            s = perf_counter()\n",
    "            func = fn(*args, **kwargs)\n",
    "            e = perf_counter()\n",
    "            print(f\"{fn.__name__} ran in {e-s: .2f}s\")\n",
    "            return func\n",
    "        return wrapper\n",
    "    \n",
    "    \n",
    "    @timer\n",
    "    def create_lemmata(self):\n",
    "        sentence_object = SentenceSeries(self.data[self.textcol])\n",
    "        self.sents = sentence_object.sentences\n",
    "        self.all_sents = sentence_object.all_sentences.fillna(\"None\")\n",
    "        self.raw_sentences = pd.Series(sentence_object.pre_sentences.apply(lambda row: [*row.sents]).values)\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def top_words(self):\n",
    "        return Counter(chain.from_iterable(self.lemmata.values)).most_common(100)\n",
    "    \n",
    "    \n",
    "    def top_ngrams(self, nfunc):\n",
    "        '''\n",
    "         c.top_ngrams(nltk.bigrams)\n",
    "         c.top_ngrams(nltk.trigrams)\n",
    "        '''\n",
    "        all_grams = chain.from_iterable(self.lemmata.apply(lambda row : [*nfunc(row)]).values)\n",
    "        return Counter(all_grams).most_common(100)\n",
    "    \n",
    "    \n",
    "    def filter_similar(self, word):\n",
    "        filtered = self.lemmata[self.lemmata.apply(lambda x : word in x)]\n",
    "        return {x[0] for x in Counter(chain.from_iterable(filtered.values)).most_common(10)}\n",
    "    \n",
    "    @timer\n",
    "    def associative_search(self, *words, bigram=False, to_excel=False):\n",
    "        '''\n",
    "        For the time being, a fairly convoluted implementation of the actual associative search engine.\n",
    "        '''\n",
    "        all_sents_df = pd.DataFrame(self.all_sents)\n",
    "        all_sents_df.columns = [self.textcol]\n",
    "        associated_words = {*chain.from_iterable([list(Word(word)().values())[0] for word in words])}\n",
    "        containing_df = all_sents_df[all_sents_df[self.textcol].apply(lambda x:  any([str(y).lower() in associated_words for y in x]))]\n",
    "        positions = [*set(containing_df.index)]\n",
    "        recombined_df = pd.DataFrame(containing_df.groupby(containing_df.index)[self.textcol].apply(lambda x:','.join(x.astype(str))))\n",
    "        recombined_df[\"tag\"] = recombined_df[self.textcol]\n",
    "        del recombined_df[self.textcol]\n",
    "        recombined_df = pd.concat([self.data.take(positions), recombined_df], axis=1)\n",
    "        recombined_df[\"tag\"] = recombined_df[\"tag\"].apply(lambda row: row.lower())\\\n",
    "                                                   .apply(lambda row: [x for x in associated_words if x in row])\n",
    "        \n",
    "        if to_excel:\n",
    "            recombined_df.to_excel(f\"{'_'.join(words)}_associated_search_res.xlsx\")\n",
    "    \n",
    "#         df = pd.concat([recombined_df, self.sents.apply(lambda x: [*chain.from_iterable(x)])], axis=1)\n",
    "\n",
    "\n",
    "        return recombined_df\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Corpus.from_csv(\"leroy\", sample=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_lemmata ran in  10.97s\n"
     ]
    }
   ],
   "source": [
    "c.create_lemmata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec synonyms: ['szafa', 'komoda', 'kredens', 'szuflada', 'skrzynka', 'schowek', 'szafeczka', 'pÃ³Å‚ka', 'stojak', 'lodÃ³wka', 'komÃ³dka', 'toaletka', 'pÃ³Å‚eczka', 'kasetka', 'przegrÃ³dka', 'skrzyneczka', 'regaÅ‚', 'bieliÅºniarka', 'pudeÅ‚ko', 'wieszak']\n",
      "--------------------------------------------------\n",
      "Scraped synonyms: ['schowek', 'kredens', 'gabinet', 'szafy', 'szafie', 'garderoba', 'szafÄ™', 'szuflada', 'komodzie', 'klozecie', 'firma', 'biuro', 'magazyn', 'skrytka', 'praktyka', 'klinika']\n",
      "--------------------------------------------------\n",
      "biuro\n",
      "gabinet\n",
      "garderoba\n",
      "klinika\n",
      "klozecie\n",
      "komodzie\n",
      "kredens\n",
      "magazyn\n",
      "praktyka\n",
      "schowek\n",
      "skrytka\n",
      "szafie\n",
      "szafka\n",
      "szafy\n",
      "szafÄ™\n",
      "szuflada\n",
      "--------------------------------------------------\n",
      "associative_search ran in  0.61s\n"
     ]
    }
   ],
   "source": [
    "result = c.associative_search(\"szafka\", bigram=False, to_excel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TreÅ›Ä‡ wypowiedzi</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Post 17. Dzisiaj napiszÄ™ o powstawaniu mebli kuchennych :) ğŸ§‘â€ğŸ³ğŸ‘©â€ğŸ³. ZrobiliÅ›my konstrukcje- boki ...</td>\n",
       "      <td>[szuflada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Zdecydowanie odradzam, meble Åºle wymierzoneniedopasowane, szary miÄ™dzy szafkami dolna i gÃ³rnÄ… a ...</td>\n",
       "      <td>[szafka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>DzieÅ„ dobry.\\n\\nDzisiaj mam dla Was magicznÄ… sypialniÄ™ w ktÃ³rej krÃ³luje tapeta od UbierzSwojeÅšci...</td>\n",
       "      <td>[szafka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Witam,\\n\\nDzisiaj mam dla Was przepiÄ™kna wizualizacjÄ™ sypialni przy uÅ¼yciu tapety od UbierzSwoje...</td>\n",
       "      <td>[szafka, garderoba]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Witajcie w Naszej bajce ğŸ˜ğŸ’¥\\n\\nDÅ‚ugo zwlekaÅ‚am, Å¼eby zaÅ‚oÅ¼yÄ‡ ten profil,ale w koÅ„cu siÄ™ udaÅ‚o! Od...</td>\n",
       "      <td>[praktyka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Dobry WieczÃ³r Obecnym!\\n\\nW dzisiejszym poÅ›cie chce Wam przedstawiÄ‡ a raczej pochwaliÄ‡ siÄ™ jak n...</td>\n",
       "      <td>[szafka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>ğŸŒ Od 17 paÅºdziernika bÄ™dziecie mogli zobaczyÄ‡ w CMWÅ wystawÄ™ â€Ziemia (P)oddanaâ€. Stanowi ona pod...</td>\n",
       "      <td>[magazyn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>Dawno takiego urwania gÅ‚owy nie miaÅ‚am i to nie zwiÄ…zanego z mojÄ… pracÄ… ğŸ˜Š po nowym roku czekajÄ… ...</td>\n",
       "      <td>[szafka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>Najlepiej meble na wymiar. Meble takie bÄ™dÄ… oczywiÅ›cie droÅ¼sze od mebli gotowych, ale kuchni nie...</td>\n",
       "      <td>[szafka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>Promocja Noworoczna ğŸ¥‚\\nâ €\\nMieszkania w budynkach 6 i 7 sÄ… teraz taÅ„sze nawet o 17 000 zÅ‚ âš ï¸\\nâ €\\n...</td>\n",
       "      <td>[biuro]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>Nowy Rok rozpoczynamy skoÅ„czonÄ… Å‚azienkÄ…! ğŸ›€\\n\\nZachowaliÅ›my tu klimat bardziej nowoczesny. KrÃ³lu...</td>\n",
       "      <td>[szafka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>Kolejny element ukÅ‚adanki pojawiÅ‚ siÄ™ w gabinecie. Chodzi tu oczywiÅ›cie o dywan, ktÃ³rego znalezi...</td>\n",
       "      <td>[gabinet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>A tutaj maÅ‚a zapowiedÅº zmian w kuchni ğŸ’™\\nPo drugiej stronie zmieniÅ‚o siÄ™ wszystko, czekam tylko ...</td>\n",
       "      <td>[szafka]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>U nas po Tesco bÄ™dzie niedÅ‚ugo Leroy Merlin Soprano 09/07/2020 10:11 W czerwcu byÅ‚o jeszcze 5Kap...</td>\n",
       "      <td>[kredens]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>DziÅ› w krÃ³tkim wideo przedstawiamy Wam 10-letniÄ… MichalinÄ™ â€“ jednÄ… z kuratorek wystawy â€Ziemia (...</td>\n",
       "      <td>[magazyn]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                        TreÅ›Ä‡ wypowiedzi  \\\n",
       "33   Post 17. Dzisiaj napiszÄ™ o powstawaniu mebli kuchennych :) ğŸ§‘â€ğŸ³ğŸ‘©â€ğŸ³. ZrobiliÅ›my konstrukcje- boki ...   \n",
       "40   Zdecydowanie odradzam, meble Åºle wymierzoneniedopasowane, szary miÄ™dzy szafkami dolna i gÃ³rnÄ… a ...   \n",
       "155  DzieÅ„ dobry.\\n\\nDzisiaj mam dla Was magicznÄ… sypialniÄ™ w ktÃ³rej krÃ³luje tapeta od UbierzSwojeÅšci...   \n",
       "182  Witam,\\n\\nDzisiaj mam dla Was przepiÄ™kna wizualizacjÄ™ sypialni przy uÅ¼yciu tapety od UbierzSwoje...   \n",
       "226  Witajcie w Naszej bajce ğŸ˜ğŸ’¥\\n\\nDÅ‚ugo zwlekaÅ‚am, Å¼eby zaÅ‚oÅ¼yÄ‡ ten profil,ale w koÅ„cu siÄ™ udaÅ‚o! Od...   \n",
       "292  Dobry WieczÃ³r Obecnym!\\n\\nW dzisiejszym poÅ›cie chce Wam przedstawiÄ‡ a raczej pochwaliÄ‡ siÄ™ jak n...   \n",
       "300  ğŸŒ Od 17 paÅºdziernika bÄ™dziecie mogli zobaczyÄ‡ w CMWÅ wystawÄ™ â€Ziemia (P)oddanaâ€. Stanowi ona pod...   \n",
       "405  Dawno takiego urwania gÅ‚owy nie miaÅ‚am i to nie zwiÄ…zanego z mojÄ… pracÄ… ğŸ˜Š po nowym roku czekajÄ… ...   \n",
       "458  Najlepiej meble na wymiar. Meble takie bÄ™dÄ… oczywiÅ›cie droÅ¼sze od mebli gotowych, ale kuchni nie...   \n",
       "526  Promocja Noworoczna ğŸ¥‚\\nâ €\\nMieszkania w budynkach 6 i 7 sÄ… teraz taÅ„sze nawet o 17 000 zÅ‚ âš ï¸\\nâ €\\n...   \n",
       "552  Nowy Rok rozpoczynamy skoÅ„czonÄ… Å‚azienkÄ…! ğŸ›€\\n\\nZachowaliÅ›my tu klimat bardziej nowoczesny. KrÃ³lu...   \n",
       "567  Kolejny element ukÅ‚adanki pojawiÅ‚ siÄ™ w gabinecie. Chodzi tu oczywiÅ›cie o dywan, ktÃ³rego znalezi...   \n",
       "640  A tutaj maÅ‚a zapowiedÅº zmian w kuchni ğŸ’™\\nPo drugiej stronie zmieniÅ‚o siÄ™ wszystko, czekam tylko ...   \n",
       "704  U nas po Tesco bÄ™dzie niedÅ‚ugo Leroy Merlin Soprano 09/07/2020 10:11 W czerwcu byÅ‚o jeszcze 5Kap...   \n",
       "818  DziÅ› w krÃ³tkim wideo przedstawiamy Wam 10-letniÄ… MichalinÄ™ â€“ jednÄ… z kuratorek wystawy â€Ziemia (...   \n",
       "\n",
       "                     tag  \n",
       "33            [szuflada]  \n",
       "40              [szafka]  \n",
       "155             [szafka]  \n",
       "182  [szafka, garderoba]  \n",
       "226           [praktyka]  \n",
       "292             [szafka]  \n",
       "300            [magazyn]  \n",
       "405             [szafka]  \n",
       "458             [szafka]  \n",
       "526              [biuro]  \n",
       "552             [szafka]  \n",
       "567            [gabinet]  \n",
       "640             [szafka]  \n",
       "704            [kredens]  \n",
       "818            [magazyn]  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[[\"TreÅ›Ä‡ wypowiedzi\", 'tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b88c8b3aef7e8088e0963b11b034c347091d77e3519f3a60d33afcaa6580504b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
